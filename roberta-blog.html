<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Building ML for Teacher Selection: When Getting It Wrong Means Leaving Kids Behind</title>
    <style>
        :root {
            --teal: #5eadbd;
            --teal-light: #e8f4f6;
            --gray: #555;
            --gray-light: #777;
            --gray-lighter: #e0e0e0;
            --black: #2c2c2c;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', sans-serif;
            line-height: 1.7;
            color: var(--black);
            background: #ffffff;
            padding-top: 70px;
        }

        nav {
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            background: rgba(255, 255, 255, 0.95);
            backdrop-filter: blur(10px);
            border-bottom: 1px solid var(--gray-lighter);
            z-index: 1000;
        }

        nav .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 20px 40px;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        nav .logo {
            font-size: 1.1rem;
            font-weight: 600;
            color: var(--black);
            text-decoration: none;
        }

        nav .nav-links {
            display: flex;
            gap: 30px;
        }

        nav .nav-links a {
            color: var(--gray);
            text-decoration: none;
            font-size: 0.95rem;
            transition: color 0.3s;
        }

        nav .nav-links a:hover {
            color: var(--teal);
        }

        .container {
            max-width: 800px;
            margin: 0 auto;
            padding: 0 40px;
        }

        .back-link {
            padding: 80px 0 30px;
        }

        .back-link a {
            color: var(--teal);
            text-decoration: none;
            font-size: 0.95rem;
            display: inline-flex;
            align-items: center;
            gap: 8px;
        }

        .back-link a:hover {
            text-decoration: underline;
        }

        .post-header {
            padding: 20px 0 50px;
            border-bottom: 1px solid var(--gray-lighter);
        }

        .post-header h1 {
            font-size: 2.5rem;
            line-height: 1.2;
            margin-bottom: 20px;
            color: var(--black);
        }

        .post-meta {
            color: var(--gray-light);
            font-size: 0.95rem;
        }

        .post-content {
            padding: 60px 0;
        }

        .post-content h2 {
            font-size: 1.8rem;
            margin: 50px 0 25px;
            color: var(--black);
        }

        .post-content h3 {
            font-size: 1.3rem;
            margin: 40px 0 20px;
            color: var(--black);
        }

        .post-content p {
            margin-bottom: 20px;
            font-size: 1.05rem;
        }

        .post-content ul, .post-content ol {
            margin: 20px 0 20px 25px;
        }

        .post-content li {
            margin-bottom: 10px;
            font-size: 1.05rem;
        }

        .post-content strong {
            color: var(--black);
            font-weight: 600;
        }

        .post-content em {
            font-style: italic;
        }

        .post-content pre {
            background: #f5f5f5;
            padding: 20px;
            border-radius: 6px;
            overflow-x: auto;
            margin: 25px 0;
            font-size: 0.9rem;
            border: 1px solid var(--gray-lighter);
        }

        .post-content code {
            font-family: 'Monaco', 'Courier New', monospace;
        }

        .post-content table {
            width: 100%;
            border-collapse: collapse;
            margin: 30px 0;
        }

        .post-content table th,
        .post-content table td {
            padding: 12px;
            text-align: left;
            border-bottom: 1px solid var(--gray-lighter);
        }

        .post-content table th {
            background: var(--teal-light);
            font-weight: 600;
            color: var(--black);
        }

        .callout-box {
            background: linear-gradient(135deg, #ffebee 0%, #ffcdd2 100%);
            border: 3px solid #e74c3c;
            border-radius: 12px;
            padding: 30px 40px;
            margin: 40px 0;
            text-align: center;
        }

        .callout-box h3 {
            color: #c0392b;
            font-size: 1.5rem;
            margin: 0 0 20px 0;
            letter-spacing: 1px;
        }

        .callout-box p {
            font-size: 1.15rem;
            line-height: 1.6;
            margin-bottom: 12px;
            color: #2c3e50;
        }

        .callout-box p:last-child {
            margin-bottom: 0;
            font-style: italic;
            font-size: 1.05rem;
        }

        .callout-box .emphasis {
            font-weight: 700;
            color: #2c3e50;
        }

        .kicker {
            background: var(--teal-light);
            border-left: 4px solid var(--teal);
            padding: 25px 30px;
            margin: 50px 0 30px;
            font-size: 1.2rem;
            font-weight: 500;
            font-style: italic;
            color: var(--black);
            line-height: 1.6;
        }

        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 30px auto;
            border-radius: 8px;
        }

        @media (max-width: 768px) {
            body {
                padding-top: 60px;
            }

            nav .container {
                padding: 15px 25px;
            }

            .container {
                padding: 0 25px;
            }

            .post-header h1 {
                font-size: 2rem;
            }

            .post-content h2 {
                font-size: 1.5rem;
            }

            .post-content h3 {
                font-size: 1.2rem;
            }

            .callout-box {
                padding: 25px 20px;
            }
        }
    </style>
</head>
<body>
    <!-- Navigation -->
    <nav>
        <div class="container">
            <a href="index.html" class="logo">Niranjan Prakash</a>
            <div class="nav-links">
                <a href="index.html#about">About</a>
                <a href="index.html#experience">Experience</a>
                <a href="index.html#projects">Projects</a>
                <a href="blog.html">Blog</a>
            </div>
        </div>
    </nav>

    <!-- Back Link -->
    <div class="container">
        <div class="back-link">
            <a href="blog.html">← Back to Blog</a>
        </div>
    </div>

    <!-- Post Header -->
    <div class="container">
        <div class="post-header">
            <h1>Building ML for Teacher Selection: When Getting It Wrong Means Leaving Kids Behind</h1>
            <div class="post-meta">
                <span>Niranjan Prakash | December 2025</span>
            </div>
        </div>
    </div>

    <!-- Post Content -->
    <article class="post-content">
        <div class="container">
            
            <!-- A. Introduction -->
            <p>I spent two years as a Teach For India Fellow in Ahmedabad, teaching Math and Science to 200 students in a low-income private school. It was among the most challenging experiences of my life. I was managing limited resources, navigating precarious relationships with parents and school administration, dealing with the chaos of being a first-time teacher at the age of 22, maybe 5 years older than my students at that point.</p>

            <p>Some days, I genuinely questioned whether I could keep going. But I did. Because leaving mid-year would have meant those 200 students losing yet another teacher, falling further behind and leaving them with another adult who broke their promise.</p>

            <p>Not every Fellow makes it through the two-year commitment. And when they don't, the students pay the price. Disrupted learning, lost relationships, and the message that maybe they're not worth sticking around for.</p>

            <p>The data backs this up: <strong>Fellows who score low on a trait, specifically 'grit', are nearly twice as likely to leave before completing their commitment</strong> compared to those who meet the bar. (Duckworth et al., 2007)</p>

            <h3>The Research Question</h3>
            <p>Can we build a machine learning model that identifies at-risk candidates from their application essays to focus manual evaluators on the cases that matter most?</p>

            <p>This blog goes over a fine-tuned RoBERTa model that is capable of processing 20,000+ applications anually, achieving 63% recall on at-risk candidates while reducing manual screening workload by 60%. More importantly, it demonstrates how optimizing for the right metric, not the highest accuracy, can create real-world impact when human context shapes technical decisions.</p>

            <!-- B. Background & Related Work -->
            <h2>Background & Related Work</h2>

            <p>Teacher attrition is a well-documented challenge in alternative certification programs. Research by Teach For America found that grit—defined as perseverance and passion for long-term goals—is among the strongest predictors of teacher persistence (Duckworth et al., 2007). However, evaluating grit at scale remains challenging.</p>

            <p>Natural Language Processing has shown promise in educational contexts, particularly in automated essay scoring (Shermis & Burstein, 2013). Recent transformer-based models like BERT and RoBERTa have achieved near-human performance on various NLP tasks (Liu et al., 2019). However, most work focuses on maximizing overall accuracy rather than optimizing for specific business outcomes.</p>

            <p><strong>The Gap:</strong> Existing approaches don't address the asymmetric cost structure of educational selection, where false positives (accepting candidates who leave) have far greater consequences than false negatives (flagging strong candidates for review). This work explicitly optimizes for catching at-risk candidates, even at the cost of overall accuracy.</p>

            <h3>Why Not ChatGPT?</h3>
            <p>Before diving into the technical approach, it's worth addressing an obvious question: why build a custom model when ChatGPT exists?</p>

            <p>Three reasons:</p>
            <ul>
                <li><strong>Cost:</strong> Processing 20,000 essays through API calls adds up quickly—thousands of dollars per application cycle</li>
                <li><strong>Speed:</strong> API latency creates bottlenecks during deadline weeks when hundreds of essays arrive daily</li>
                <li><strong>Control:</strong> TFI has 14,000 labeled essays from past cohorts—fine-tuning gives precise control over predictions and allows us to optimize for our specific task</li>
            </ul>

            <!-- C. Data -->
            <h2>Data</h2>

            <h3>Dataset Description</h3>
            <p>The dataset comprises 14,000 labeled essays from Teach For India fellowship applications spanning 2021-2023. Each essay responds to the prompt: <em>"Describe a time where you had to push yourself and overcome challenges to succeed. This commitment could be from your professional or extracurricular experiences."</em></p>

            <p>Additional constraints on the experience: (a) it must not be a personal experience, and (b) must be at least a month-long commitment.</p>

            <p>Essays are scored on a 1-5 rubric by trained evaluators:</p>
            <ul>
                <li><strong>Score 1:</strong> Mentions an experience which does not meet requirements of the prompt</li>
                <li><strong>Score 2:</strong> Has potential to meet the bar but is missing information or a low potential commitment</li>
                <li><strong>Score 3:</strong> Demonstrates solid grit through meaningful challenges</li>
                <li><strong>Score 4:</strong> Shows exceptional persistence with significant obstacles</li>
                <li><strong>Score 5:</strong> Outstanding evidence across multiple dimensions</li>
            </ul>

            <p>The data was split 80/20 into training (11,200) and test sets (2,800). The class distribution was severely imbalanced:</p>

            <img src="images/class_distribution_roberta.png" alt="Bar chart showing class distribution - Score 2: 42%, Score 3: 31%, Score 1: 21%, Score 4: 6%, Score 5: 0.5%" style="max-width: 700px;">

            <ul>
                <li>Score 2: 6,000 essays (42%)</li>
                <li>Score 3: 4,422 essays (31%)</li>
                <li>Score 1: 3,006 essays (21%)</li>
                <li>Score 4: 806 essays (6%)</li>
                <li>Score 5: 70 essays (0.5%)</li>
            </ul>

            <p>The severe imbalance toward score 2 is not a data collection artifact—it reflects reality. Most applicants demonstrate baseline grit but fall short of exceptional persistence. This distribution became a feature, not a bug.</p>

            <!-- D. Exploratory Data Analysis -->
            <h2>Exploratory Data Analysis</h2>

            <p>Before modeling, I conducted exploratory analysis to understand what distinguishes high-scoring from low-scoring essays.</p>

            <h3>Essay Length Distribution</h3>

            <img src="images/essay_length_boxplot.png" alt="Box plot of essay length by score, showing clear separation - Score 1: 274 words, Score 2: 259 words, Score 3: 318 words, Score 4: 431 words, Score 5: 538 words" style="max-width: 700px;">

            <p>A striking pattern emerged: essay length correlates strongly with score. Score 1 essays averaged 287 words, score 2 averaged 412 words, while scores 3-5 averaged 520+ words. This wasn't about hitting word counts—longer essays provided more complete narratives with context, obstacles, actions, and outcomes. Shorter essays often lacked crucial elements of storytelling.</p>

            <h3>Preliminary Finding: Content vs. Structure</h3>
            <p>I hypothesized that essay topics would be the strongest predictor—teaching experience would outperform, say, sports achievements. To test this, I applied Latent Dirichlet Allocation (LDA) to extract 8 topics from the corpus.</p>

            <img src="images/topic_distribution_heatmap.png" alt="Heatmap showing topic distribution across score levels - Topics include: Classroom Teaching, Academic Challenges, Sports/Athletics, Social Initiatives, Workplace Projects, Leadership Roles, Community Service, Team Management" style="max-width: 700px;">

            <p><strong>The surprising finding:</strong> Topic distribution was nearly uniform across scores. Essays about classroom teaching weren't rated higher than essays about sports or community work. What mattered was <em>how</em> candidates told their story—the completeness of narrative structure, the specificity of obstacles, and the clarity of their role—not <em>what</em> they wrote about.</p>

            <p>This informed a crucial modeling decision: don't rely on topic features. Let the transformer learn nuanced linguistic patterns that capture narrative quality rather than surface-level content.</p>


            <!-- F. TECHNICAL APPROACH -->
            <h2>Technical Approach</h2>

            <h3>The Imbalance Choice</h3>
            <p>When dealing with imbalanced classes in a classification problem, the conventional wisdom is to either generate synthetic data to boost the minority class or use balanced class weights to ensure the model accurately learns each class.</p>

            <p><strong>I did neither.</strong></p>

            <p>The purpose of this model is NOT to predict which applicants will be fellows. Its job is to predict which applicants will not be fellows, with a high degree of accuracy and confidence.</p>

            <p>Why? The fellowship is hard in a specific way. Most candidates are somewhere in the middle. Having 42% of essays scored as "2" meant the model saw tons of examples of "just meeting the bar." This imbalance encodes exactly the signal I needed: strong detection of at-risk candidates.</p>

            <h3>Model Architecture</h3>
            <p>I fine-tuned <strong>RoBERTa-base</strong> (125M parameters) with a critical architectural choice: output probability distributions instead of hard classifications.</p>

            <p><strong>Base Model:</strong> <code>roberta-base</code> (pretrained on 160GB of text)<br>
            <strong>Task Head:</strong> Linear layer mapping 768-dimensional embeddings to 5-class softmax output<br>
            <strong>Loss Function:</strong> Cross-entropy loss with modest class weights</p>

            <pre><code>Input: "I was managing a classroom of 45 students..."
↓
RoBERTa encoder (12 layers, 768-dim hidden states)
↓
Classification head
↓
Output: P(1)=12%, P(2)=68%, P(3)=18%, P(4)=2%, P(5)=0%</code></pre>

            <img src="images/roberta_architecture_simple.png" alt="RoBERTa architecture diagram showing horizontal flow from input text through encoder, pooling, classifier, to probability distribution output">

            <p>Instead of returning "This essay is a 2," the model outputs: "68% confident it's a 2, but 18% chance it's a 3." This uncertainty quantification became crucial in production.</p>

            <h3>Training Configuration</h3>
            <p><strong>Hyperparameters:</strong></p>
            <ul>
                <li>Learning rate: 2e-5 (with linear warmup over 500 steps)</li>
                <li>Batch size: 16 (gradient accumulation for effective batch size of 32)</li>
                <li>Epochs: 4</li>
                <li>Max sequence length: 512 tokens</li>
                <li>Optimizer: AdamW (weight decay: 0.01)</li>
            </ul>

            <p><strong>Class weights (square root of inverse frequency):</strong></p>
            <pre><code>weights = {1: 2.4119, 2: 0.4148, 3: 0.5716, 4: 2.5225, 5: 35.1519}</code></pre>

            <p>These modest weights give the model a slight nudge toward minority classes without aggressive rebalancing that would distort the natural distribution.</p>

            <p><strong>Training time:</strong> 3 hours on Kaggle (T4 GPU)</p>

            <h3>The Metric Choice: Recall on Low Scores</h3>
            <p>The model is not optimized for accuracy across all classes. Since the current task involves identifying low-potential applicants, we want to optimize for catching candidates at risk of leaving.</p>

            <p>This means maximizing recall on scores 1-2 (the critical class), even if it means lower overall accuracy.</p>

            <h3>Comparative Model: DeBERTa with Balanced Data</h3>
            <p>To validate the imbalance strategy, I trained a second model using <strong>DeBERTa-v3-base</strong> with balanced class distribution:</p>
            <ul>
                <li>Downsampled majority classes (scores 2-3) to 2,000 each</li>
                <li>Final dataset: 10,000 essays (vs. 14,000 for RoBERTa)</li>
                <li>Same hyperparameters otherwise</li>
            </ul>

            <p>This created a controlled comparison: Does keeping natural imbalance improve performance on the critical task?</p>

            <!-- G. RESULTS -->
            <h2>Results</h2>

            <h3>Model Comparison</h3>
            <p>I trained two models to validate the approach:</p>

            <p><strong>RoBERTa (production model):</strong></p>
            <ul>
                <li>14k essays, natural imbalance</li>
                <li>49% overall accuracy</li>
                <li><strong>63% recall on score 1-2</strong> (catching at-risk candidates)</li>
            </ul>

            <p><strong>DeBERTa (balanced alternative):</strong></p>
            <ul>
                <li>10k essays, rebalanced distribution</li>
                <li>52% overall accuracy ✓</li>
                <li><strong>43% recall on score 1-2</strong> ✗</li>
            </ul>

            <div style="display: flex; gap: 20px; justify-content: center; align-items: flex-start; flex-wrap: wrap; margin: 30px 0;">
                <div style="flex: 1; min-width: 300px; max-width: 450px;">
                    <img src="images/confusion_matrix_roberta.png" alt="RoBERTa confusion matrix showing 63% recall on scores 1-2" style="width: 100%; margin: 0;">
                    <p style="text-align: center; font-size: 0.95rem; margin-top: 10px; color: var(--gray);"><strong>RoBERTa:</strong> Strong on scores 1-2</p>
                </div>
                <div style="flex: 1; min-width: 300px; max-width: 450px;">
                    <img src="images/deberta_confusion_matrix.png" alt="DeBERTa confusion matrix showing 43% recall on scores 1-2" style="width: 100%; margin: 0;">
                    <p style="text-align: center; font-size: 0.95rem; margin-top: 10px; color: var(--gray);"><strong>DeBERTa:</strong> Better on scores 3-5</p>
                </div>
            </div>

            <p><strong>The "worse" model by traditional metrics was better for the actual task.</strong></p>

            <p>DeBERTa is better at predicting high scores, but those candidates pass through regardless. RoBERTa is <strong>20 percentage points better</strong> at identifying low scores—specifically scores of 1 and 2, which is the critical boundary here.</p>

            <p><strong>Takeaway:</strong> Optimize for the business problem, not the ML metric. My prior experience as a Fellow gave me the crucial context to know which metric actually mattered.</p>


            <!-- H. PRODUCTION & IMPACT -->
            <h2>Production Deployment & Impact</h2>

            <h3>Deployment Architecture</h3>
            <p><strong>Infrastructure:</strong></p>
            <ul>
                <li>HuggingFace Spaces (free tier) + Flask backend</li>
                <li>Weekly batch processing: ~300 essays (regular), ~600 (deadline periods)</li>
                <li>5 months in production: 10,000-15,000 essays processed</li>
                <li>Cold start time: ~60 seconds (acceptable for batch mode)</li>
                <li>Processing speed: ~2 seconds per essay after warm-up</li>
            </ul>

            <p>Cold starts aren't an issue for batch processing. The system tolerates occasional infrastructure hiccups with fallback manual processes.</p>

            <h3>Human-in-the-Loop System</h3>
            <p>Model outputs feed into rule-based filtering combining essay scores with application data. Probability distributions enable intelligent flagging:</p>

            <p><strong>High confidence rejection:</strong></p>
            <pre><code>P(1)=88%, P(2)=9%, P(3)=2%
→ Clear low score, no review needed</code></pre>

            <p><strong>Uncertain case:</strong></p>
            <pre><code>P(1)=11%, P(2)=46%, P(3)=33%
→ Most likely a 2, but could be 3
→ Flag for human review</code></pre>

            <h3>Impact Metrics</h3>
            <p><strong>Before automation:</strong></p>
            <ul>
                <li>3 team members manually scoring essays</li>
                <li>2-10 hours per week per person depending on volume</li>
                <li>6-30 person-hours per week on initial screening</li>
            </ul>

            <p><strong>After automation:</strong></p>
            <ul>
                <li>Model processes 300 essays in ~5 minutes</li>
                <li>Team focuses only on flagged cases (67%)</li>
                <li><strong>60% reduction in manual screening work</strong></li>
            </ul>

            <p><strong>More importantly:</strong> The selection team can now invest their expertise in nuanced evaluations rather than mechanical scoring.</p>

            <!-- I. PRODUCTION VALIDATION -->
            <h2>Production Validation (5 Months)</h2>

            <p>I analyzed 1,443 essays from one complete application round (August- November 2025) to validate production performance.</p>

            <p><strong>Automatic Filtering:</strong></p>
            <ul>
                <li>471 essays (33%) filtered out automatically using RoBERTa predictions</li>
                <li>Threshold: P(1) > 70% OR P(1) + P(2) > 85%</li>
                <li>These cases went straight to rejection without human review</li>
            </ul>

            <p><strong>Human Review (972 essays):</strong></p>
            <ul>
                <li>45% exact accuracy (predicted exact human score)</li>
                <li><strong>91% within-1 accuracy</strong> (within one score point)</li>
                <li>65% scored 3+ (appropriately passed)</li>
                <li>35% scored 1-2 (at-risk candidates that needed review)</li>
            </ul>

            <p>The 35% "false positive" rate (flagged for review but turned out to be low-scoring) reflects conservative design: when uncertain, flag for review rather than auto-reject. This protects against high-stakes mistakes.</p>

            <!-- J. LESSONS LEARNED -->
            <h2>Lessons Learned</h2>

            <h3>1. Task Definition > Accuracy</h3>
            <p>A model with 49% accuracy beat one with 52% because it solved the right problem. If I'd optimized for overall accuracy, I would've deployed the wrong model. My experience as a Fellow gave me the context to know that false negatives (flagging strong candidates for review) are recoverable through human evaluation, but false positives (accepting candidates who leave) directly harm students.</p>

            <h3>2. Class Imbalance as Signal</h3>
            <p>Having 42% score-2 essays reflected reality: most applicants are in the middle. The fellowship is hard—not everyone thrives, but not everyone fails. The model seeing tons of "just meeting the bar" examples made it better at distinguishing score 2 (at risk) from score 3 (likely to persist).</p>

            <p><strong>Caveat:</strong> I didn't run full ablation studies comparing different balancing strategies due to compute constraints. The DeBERTa comparison confounds sample size (10k vs. 14k) with balancing strategy. However, production results validate the approach: 5 months of real-world use show the model generalizes well.</p>

            <h3>3. Probability Distributions > Hard Classifications</h3>
            <p>Outputting probability distributions enabled confidence-based filtering, natural integration with human review workflows, and transparency about uncertainty—critical when decisions affect people's futures. Even humans disagree on borderline cases. A model that says "I'm uncertain" is more valuable than one forcing predictions.</p>

            <h3>4. Free Hosting for Nonprofit Budgets</h3>
            <p>HuggingFace Spaces free tier works for batch processing despite limitations (cold starts, potential downtime). Design your system to tolerate constraints: batch weekly instead of real-time, accept occasional cold starts, maintain fallback manual processes. Not every ML system needs enterprise infrastructure.</p>

            <h3>5. Human Context Shapes Technical Decisions</h3>
            <p>Every technical choice—keeping class imbalance, optimizing for recall on low scores, outputting probability distributions—was informed by understanding the human impact. I knew what it felt like to want to quit. I knew what happened to students when teachers left. That context shaped the model in ways purely technical optimization never would.</p>

            <!-- K. CONCLUSION & FUTURE WORK -->
            <h2>Conclusion & Future Work</h2>

            <p>This project demonstrates that optimizing for the right metric—not the highest accuracy—can create meaningful real-world impact. A 49% accurate model that catches 63% of at-risk candidates is more valuable than a 52% accurate model that catches only 43%, because it solves the actual problem.</p>

            <h3>What's Next</h3>
            <p>The model has been running successfully for 5 months. Next steps:</p>

            <ul>
                <li><strong>Production monitoring:</strong> Collecting feedback from evaluators on flagged cases to measure real-world calibration and adjust confidence thresholds based on attrition data from recent cohorts</li>
                <li><strong>Retention validation:</strong> Link model predictions to actual attrition data from 2024-2026 cohorts to validate predictive power (this requires 2+ years of follow-up data)</li>
                <li><strong>Expanding scope:</strong> Applying similar uncertainty-based approaches to other traits in the selection rubric, potentially building ensemble models that combine predictions across multiple dimensions</li>
            </ul>

            <p>The ultimate validation will come from tracking whether Fellows the model flagged as low-risk actually complete their commitments. That data is still two years away. Until then, the model serves its purpose: focusing expert human judgment where it matters most.</p>

            <p>Because at the end of the day, this isn't about ML metrics. It's about making sure that when a teacher walks into a classroom in Ahhedabad or Mumbai or Delhi, they have the grit to stick it out—so 200 kids don't lose another adult who broke their promise.</p>

            <!-- L. REFERENCES -->
            <h2>References</h2>

            <p><strong>Academic Literature:</strong></p>
            <ul>
                <li>Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. <em>Proceedings of NAACL-HLT 2019</em>, 4171-4186.</li>
                <li>Duckworth, A. L., Peterson, C., Matthews, M. D., & Kelly, D. R. (2007). Grit: Perseverance and passion for long-term goals. <em>Journal of Personality and Social Psychology, 92</em>(6), 1087-1101.</li>
                <li>He, P., Liu, X., Gao, J., & Chen, W. (2021). DeBERTa: Decoding-enhanced BERT with Disentangled Attention. <em>Proceedings of ICLR 2021</em>.</li>
                <li>Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., ... & Stoyanov, V. (2019). RoBERTa: A Robustly Optimized BERT Pretraining Approach. <em>arXiv preprint arXiv:1907.11692</em>.</li>
                <li>Shermis, M. D., & Burstein, J. (Eds.). (2013). <em>Handbook of automated essay evaluation: Current applications and new directions</em>. Routledge.</li>
            </ul>

            <p><strong>Software & Tools:</strong></p>
            <ul>
                <li>HuggingFace Transformers (v4.30.0): <a href="https://huggingface.co/docs/transformers" target="_blank">https://huggingface.co/docs/transformers</a></li>
                <li>scikit-learn (v1.3.0): <a href="https://scikit-learn.org" target="_blank">https://scikit-learn.org</a></li>
                <li>PyTorch (v2.0.0): <a href="https://pytorch.org" target="_blank">https://pytorch.org</a></li>
            </ul>

            <p><strong>Data:</strong></p>
            <p>Teach For India fellowship application essays (2021-2023), used with organizational permission. Data not publicly available to protect applicant privacy.</p>

            <!-- M. KICKER -->
            <div class="kicker">
                "All models are wrong, but some are useful."
                <br>— George E. P. Box
            </div>

        </div>
    </article>
</body>
</html>